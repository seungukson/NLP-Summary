K-means
E와 M을기억하라.
Expectation. 다양한 거리계산방법을 이용(Euclidean기본)한 거리를 이용. 임의의 중심점 K개를 중심으로 가까운것들을 clustering
Maximization. K개의 cluster들을 기준으로 그 중심점들을 재설정(중심점이동).
---------Loop---------

K-means++
중심점의 초기값을 영-리하게
1) 첫번째 중심점은 임의로 선택
2) 첫번째 중심점으로부터 거리가 먼 데이터가 중심점으로 선택될 확률을 높여 E-M
다음 중심점은 1번째중심점과의 거리+2번째중심점과의 거리를 합한 값을 


Q1) 굳이 확정적으로 안하고 확률로 하는이유?
Q2) 결과는 같고 속도차이만??
 

Hierarchical clustering - 계산량이 많아 K-means보다 느리다.
 - 병합 클러스터링. 하나로 시작해 눈덩이처럼 붙여나감
데이터 개수만큼의 클러스터링.N^2.
클러스터 간 모든 유사도를 계산..
하나일때도 그렇지만 특히 병합클러스터일 때는 클러스터간 비교방법을 여러가지 설정할 수 있는데,
그 방법 4개가
CSCA
centroid
single
complete
average


Principal Component Analysis (PCA) : x-features간의 차원 축소가 목적. 스케일러를 통해 스케일링을 반드시해줘야한다.그게핵심.
데이터의 특성을 가장 잘 나타내주는 벡터를 찾는다.(분산을 가장 크게 갖는 벡터)
그 축을 찾는다.분산: 데이터가 흩어져있는 정도.
다음 PCV는 이전 벡터들에 직교하는 축이면서, 분산을 크게 가져가는벡터.(2차원이상)
 차원축소언제해? 학습이전의 전처리과정 / 데이터의 시각화.
rasso와 개념자체는 비슷할 수 있으나 X-feature의 선택과  new-feature를 생성해 낸다는차이가있다.
차원축소의 단점은.. 그 features의 실의미를 파악하기는 힘들다는것.
그렇다는건. 실세계에서 영향을 주는 요소를 명확히 파악할수없다는것.. 


Non-negative Matrix Factorization (NMF)

CNN- convolution neural network

Q) cnn이 dropout을 통해 계산복잡도를 줄인다고했는데 결과적으로시간이많이걸린다는 말은

많은데이터가 필요하고 연상총량이 많아진다는 뜻?

일종의 필터링을 거친 후 분류한다면 좀더 나은결과를 얻게 될 것이다..
(분류하기 좋은필터링, 안좋은필터링이 될수도있으니.. 어떤필터를쓰느냐가 중요할것.)
main idea : 학습시켜야하는 대상: filter
원본데이터보다 분류가 잘되는 데이터를 생성하는 필터를 만들자!

origin ----(filter)----->  feature map
filtering - classification


Q) filter의 크기를 줄이자.왜??계산복잡도가줄어든다는데..왜?

convolution 곱.작은필터가 이미지를 돌면서 값을 뽑아내도록 설계.
elementwise곱
필터의사이즈(size)      는 우리가 정할수 있다.
몇칸씩이동할꺼야?(stride) 도 정할수 있다.
-> feature-map의크기가 정해진다. 

바이어스도 존재하긴한다..
이미지가 컬러라면?? R필터 G필터 B필터.따로사용.
채널이 여러개라면 채널갯수의 필터가 필요.
하지만 피쳐맵은 하나.. 각필터링을 거친값을 더해서 피쳐맵으로 만들어 사용.