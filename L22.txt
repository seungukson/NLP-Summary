K-means
E와 M을기억하라.
Expectation. 다양한 거리계산방법을 이용(Euclidean기본)한 거리를 이용. 임의의 중심점 K개를 중심으로 가까운것들을 clustering
Maximization. K개의 cluster들을 기준으로 그 중심점들을 재설정(중심점이동).
---------Loop---------

K-means++
중심점의 초기값을 영-리하게
1) 첫번째 중심점은 임의로 선택
2) 첫번째 중심점으로부터 거리가 먼 데이터가 중심점으로 선택될 확률을 높여 E-M
다음 중심점은 1번째중심점과의 거리+2번째중심점과의 거리를 합한 값을 


Q1) 굳이 확정적으로 안하고 확률로 하는이유?
Q2) 결과는 같고 속도차이만??
 

Hierarchical clustering - 계산량이 많아 K-means보다 느리다.
 - 병합 클러스터링. 하나로 시작해 눈덩이처럼 붙여나감
데이터 개수만큼의 클러스터링.N^2.
클러스터 간 모든 유사도를 계산..
하나일때도 그렇지만 특히 병합클러스터일 때는 클러스터간 비교방법을 여러가지 설정할 수 있는데,
그 방법 4개가
CSCA
centroid
single
complete
average


Principal Component Analysis (PCA) : x-features간의 차원 축소가 목적. 스케일러를 통해 스케일링을 반드시해줘야한다.그게핵심.
데이터의 특성을 가장 잘 나타내주는 벡터를 찾는다.(분산을 가장 크게 갖는 벡터)
그 축을 찾는다.분산: 데이터가 흩어져있는 정도.
다음 PCV는 이전 벡터들에 직교하는 축이면서, 분산을 크게 가져가는벡터.(2차원이상)
 차원축소언제해? 학습이전의 전처리과정 / 데이터의 시각화.
rasso와 개념자체는 비슷할 수 있으나 X-feature의 선택과  new-feature를 생성해 낸다는차이가있다.
차원축소의 단점은.. 그 features의 실의미를 파악하기는 힘들다는것.
그렇다는건. 실세계에서 영향을 주는 요소를 명확히 파악할수없다는것.. 


Non-negative Matrix Factorization (NMF)

CNN- convolution neural network

Q) cnn이 dropout을 통해 계산복잡도를 줄인다고했는데 결과적으로시간이많이걸린다는 말은

많은데이터가 필요하고 연산총량이 많아진다는 뜻?

일종의 필터링을 거친 후 분류한다면 좀더 나은결과를 얻게 될 것이다..
(분류하기 좋은필터링, 안좋은필터링이 될수도있으니.. 어떤필터를쓰느냐가 중요할것.)
main idea : 학습시켜야하는 대상: filter
원본데이터보다 분류가 잘되는 데이터를 생성하는 필터를 만들자!

origin ----(filter)----->  feature map
filtering - classification


Q) filter의 크기를 줄이자.왜??계산복잡도가줄어든다는데..왜?

convolution 곱.작은필터가 이미지를 돌면서 값을 뽑아내도록 설계.
elementwise곱
필터의사이즈(size)      는 우리가 정할수 있다.
몇칸씩이동할꺼야?(stride) 도 정할수 있다.
-> feature-map의크기가 정해진다. 

바이어스도 존재하긴한다..
이미지가 컬러라면?? R필터 G필터 B필터.따로사용.
채널이 여러개라면 채널갯수의 필터가 필요.
하지만 피쳐맵은 하나.. 각필터링을 거친값을 더해서 피쳐맵으로 만들어 사용.

pooling:일종의 차원축소?
대표값을 뽑는 방법에도 여러가지가 있다.
max pool.큰값뽑는방법.
계산복잡도가 줄어든다.

padding : 컨볼루젼과정/풀링과정에서 사이즈손실을 막기위해
filtering : convolution곱 -> pulling
convolution 2번하고 풀링해도되고. 맘대로. 비율에 따라 결과값이다르다.

Q)convolution 2번한다는건 필터를 2개써서 한다는?

filtering결과를가지고 fully connected NN을 이용해 분류


RNN - Recurrent Neural Network
하나의 머신러닝 결과값이 다시 인풋h가 된다..?
그러나 마지막 학습 결과로 러닝이되는순간 모두다 가중치가 바뀌므로..결과가이상해짐.

LSTM - RNN upgrade
long term memory...<-RNN에서사용되었던?
short term memory..<- 짧게기억하는

AE.Auto Encoding

multi-layer를 통해나온 output layer의 노드갯수는 input layer의 갯수와같다.

x와 닮은 x'을 얻기위해서 사용.: 클러스터링/비지도변환으로 나뉘는데.
비지도변환: 차원축소 //특성추출
특성추출할때 x와닮은 x'을얻기위해...
데이터가부족하다면? 그때이걸쓴다.
input feature와 output feature가 같다면overfitting
->restrict boltzmann machine과 비슷하잖아? ㅇㅋ
weight의 초기값을 줄때도 씀
가운데기준으로 왼쪽웨이트와 오른쪽 웨이트값을 같게두고 하나학습되면반대도학습되게
할수있다.
Hidden Layer의 feature의 갯수가 원 feature보다 크다면 overfitting우려가있지만,
그렇게 주고 규제를 거는방법이 많다.

G AN
generater가 오차확률을 줄이는것 = discriminator의 오차확률 1/2