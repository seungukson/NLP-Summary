복습 - 
ridge : 릿지를 그린다..가중치가 곡선을 그리며 0에 가깝게.. 
lasso : 직선을 그린다.. 가중치가 0이되어버리는경우가많다.


logistic regression
시그모이드 함수의 종류 중 하나. 시그모이드 함수의 x인자에 하나의 함수를 넣음. 
즉, 시그모이드함수와의 합성함수로 classfication regression모델을 표현할 수 있다.

이렇게 합성하게 되면,,,, non-convex..즉 hypothesis가 복잡해지는데.. 복잡해지는거하면물론되지만..

다른방법이 있다?convex하게 만들어서 이동하는 방향을 정해 optimization을 쉽게하는방법..

cost = case: y = 1 : -log(h(x))
             y = 0 : -log(1-h(x))

-> cost= y*(-log(h(x))+(1-y)(-log(1-log(h(x))) 


그래서 우리가 찾고자 하는 그것... 그 선...의 방정식?

WX=0을 만족하는 weight set(W)이 coef로 이루어진 방정식이다... 



softmax?
이진 분류가 아닌 다중 분류 모형의 경우 일대다 방식으로 확률계산을 따로 해서 확률이 높은것으로 결정한다.
softmax방식은 ...?

Linear SVM : 
리니어하게 분류선을 그을 때 마진을 남겨 안정적으로..
하드마진 <                            > 소프트마진
마진오류 허용하지않겠다  --->        마진을 크게가자 
(오버피팅 가능성 up)                  (오버피팅가능성 down),
규제를 더 하자                         규제를 풀자
C값을 내리자                           C값을 올리자
마진크게가면 마진오류들이 발생. 마진오류? 서포트벡터보다 가깝거나, 선을넘어간 데이터들..

마진을 최대한 크게 가져가면서 오버피팅을 막는것이 SVM
WX=1일 때 hyperplain을 기준으로 그 안쪽에 들어오면 cost가 0..
